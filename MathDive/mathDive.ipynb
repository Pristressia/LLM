{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3626fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 24 Unique chars:  .abcdefghilmnoprstuvwxy\n",
      "step 50: loss 3.1769\n",
      "step 100: loss 3.1761\n",
      "step 150: loss 3.1752\n",
      "step 200: loss 3.1747\n",
      "step 250: loss 3.1740\n",
      "step 300: loss 3.1721\n",
      "step 350: loss 3.1717\n",
      "step 400: loss 3.1708\n",
      "tiny vmhg  avvmcaexivmnp.huigngnay ld.sytfl mar. ux.drnxh  ousvctvvhnnnesremgtplpdcc ryexpc tpcfucvbimhedrgpvvbpodgeovwrnyplwbbtw.uhpaywhwwixyc.en.aytwygrepcb.oxamplxb godbvbhyrnhlrvpt. tewmhrtrsmxwftwonpb\n"
     ]
    }
   ],
   "source": [
    "import math, numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------\n",
    "# Hyperparameters\n",
    "# -----------------------\n",
    "T = 64          # context length\n",
    "d = 64          # model dim\n",
    "d_ff = 128      # MLP hidden\n",
    "V = None        # to be set after building vocab\n",
    "lr = 1e-2\n",
    "eps = 1e-5\n",
    "\n",
    "# -----------------------\n",
    "# Data: tiny character corpus\n",
    "# -----------------------\n",
    "text = (\n",
    "    \"tiny gpt built with pure numpy. \"\n",
    "    \"this is a tiny demo to learn the math of transformers. \"\n",
    "    \"it can overfit a short text. \"\n",
    ")\n",
    "\n",
    "# Build char vocab\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "V = len(chars)\n",
    "\n",
    "def encode(s): return np.array([stoi[c] for c in s], dtype=np.int64)\n",
    "def decode(ids): return \"\".join([itos[int(i)] for i in ids])\n",
    "\n",
    "data = encode(text)\n",
    "n = len(data)\n",
    "\n",
    "# batches: random contiguous chunks\n",
    "def get_batch(bs=16):\n",
    "    ix = np.random.randint(0, n - T - 1, size=bs)\n",
    "    x = np.stack([data[i:i+T] for i in ix])             # (B,T)\n",
    "    y = np.stack([data[i+1:i+T+1] for i in ix])         # next chars\n",
    "    return x, y\n",
    "\n",
    "# -----------------------\n",
    "# Parameters (single block GPT)\n",
    "# -----------------------\n",
    "def randn(shape, scale=0.02):\n",
    "    return np.random.randn(*shape).astype(np.float32) * scale\n",
    "\n",
    "# Embeddings\n",
    "E = randn((V, d))\n",
    "P = randn((T, d))  # learnable positions\n",
    "\n",
    "# Attention projections\n",
    "W_Q = randn((d, d))\n",
    "W_K = randn((d, d))\n",
    "W_V = randn((d, d))\n",
    "W_O = randn((d, d))\n",
    "\n",
    "# RMSNorm (two of them, pre-attn and pre-mlp often used, we’ll do pre-mlp only for simplicity)\n",
    "g_attn = np.ones((d,), dtype=np.float32)\n",
    "g_mlp  = np.ones((d,), dtype=np.float32)\n",
    "\n",
    "# MLP\n",
    "W1 = randn((d, d_ff))\n",
    "W2 = randn((d_ff, d))\n",
    "\n",
    "# LM head\n",
    "WLM = randn((d, V))\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def gelu(x):\n",
    "    # tanh approximation\n",
    "    return 0.5 * x * (1.0 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def dgelu(x):\n",
    "    # derivative of tanh approximation\n",
    "    a = np.sqrt(2/np.pi)\n",
    "    t = np.tanh(a*(x + 0.044715*x**3))\n",
    "    dt_dx = (a*(1 + 3*0.044715*x**2)) * (1 - t**2)\n",
    "    return 0.5 * (1 + t) + 0.5 * x * dt_dx\n",
    "\n",
    "def rmsnorm_fwd(h, g):\n",
    "    # h: (..., d), g: (d,)\n",
    "    rms = np.sqrt(np.mean(h*h, axis=-1, keepdims=True) + eps)  # (...,1)\n",
    "    h_hat = h / rms\n",
    "    out = h_hat * g\n",
    "    cache = (h, g, rms, h_hat)\n",
    "    return out, cache\n",
    "\n",
    "def rmsnorm_bwd(dout, cache):\n",
    "    h, g, rms, h_hat = cache\n",
    "    # out = h_hat * g\n",
    "    dh_hat = dout * g\n",
    "    # h_hat = h / rms ; rms = sqrt(mean(h^2)+eps)\n",
    "    # d(h/rms) = (dh*rms - h*drms) / rms^2\n",
    "    # drms = (1/(2*rms)) * d(mean(h^2)) = (1/(2*rms)) * (2*mean(h*dh)) = mean(h*dh)/rms\n",
    "    # but we need vectorized over last dim:\n",
    "    B = h.shape[:-1]\n",
    "    d_ = h.shape[-1]\n",
    "    # mean over last dim:\n",
    "    mean_h_dh = np.mean(h * dh_hat, axis=-1, keepdims=True)\n",
    "    drms = mean_h_dh / rms\n",
    "    dh = (dh_hat / rms) - (h * drms) / (rms * rms)\n",
    "    dg = np.sum(dout * h_hat, axis=tuple(range(len(dout.shape)-1)), keepdims=False)\n",
    "    return dh, dg\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    e = np.exp(x, dtype=np.float32)\n",
    "    return e / np.sum(e, axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy(logits, targets):\n",
    "    # logits: (B,T,V), targets: (B,T)\n",
    "    B,T,V = logits.shape\n",
    "    probs = softmax(logits)\n",
    "    loss = -np.log(probs[np.arange(B)[:,None], np.arange(T)[None,:], targets]).mean()\n",
    "    return loss, probs\n",
    "\n",
    "def causal_mask(T):\n",
    "    m = np.triu(np.ones((T,T), dtype=bool), k=1)\n",
    "    return m\n",
    "\n",
    "mask = causal_mask(T)\n",
    "\n",
    "# -----------------------\n",
    "# Forward & Backward pass (one block)\n",
    "# -----------------------\n",
    "def forward_backward(x, y):\n",
    "    # x,y: (B,T)\n",
    "    B = x.shape[0]\n",
    "\n",
    "    # Embedding lookup\n",
    "    X = E[x]                       # (B,T,d)\n",
    "    H0 = X + P[None, :, :]         # add positions\n",
    "\n",
    "    # --- Attention (pre-norm optional; for simplicity we use post-norm on attn, pre-norm on MLP)\n",
    "    # Compute Q,K,V\n",
    "    Q = H0 @ W_Q                   # (B,T,d)\n",
    "    K = H0 @ W_K                   # (B,T,d)\n",
    "    Vv = H0 @ W_V                  # (B,T,d)\n",
    "\n",
    "    # Scores\n",
    "    S = (Q @ K.transpose(0,2,1)) / np.sqrt(d)  # (B,T,T)\n",
    "    S = np.where(mask[None, :, :], -1e9, S)    # causal\n",
    "\n",
    "    A = softmax(S)                 # (B,T,T)\n",
    "    O = A @ Vv                     # (B,T,d)\n",
    "    H_attn = O @ W_O               # (B,T,d)\n",
    "\n",
    "    H1 = H0 + H_attn               # residual\n",
    "\n",
    "    # --- MLP block with pre-norm (RMSNorm)\n",
    "    H1n, cache_rms = rmsnorm_fwd(H1, g_mlp)  # (B,T,d)\n",
    "    M1 = H1n @ W1                  # (B,T,d_ff)\n",
    "    M2 = gelu(M1)                  # (B,T,d_ff)\n",
    "    M3 = M2 @ W2                   # (B,T,d)\n",
    "    H2 = H1 + M3                   # residual\n",
    "\n",
    "    # LM head\n",
    "    logits = H2 @ WLM              # (B,T,V)\n",
    "\n",
    "    # Loss\n",
    "    loss, probs = cross_entropy(logits, y)\n",
    "\n",
    "    # ----------------- Backward -----------------\n",
    "    dlogits = probs\n",
    "    dlogits[np.arange(B)[:,None], np.arange(T)[None,:], y] -= 1\n",
    "    dlogits /= (B*T)\n",
    "\n",
    "    dWLM = (H2.reshape(-1, d).T @ dlogits.reshape(-1, V))\n",
    "    dH2 = dlogits @ WLM.T\n",
    "    # residual to H1 and M3\n",
    "    dH1 = dH2.copy()\n",
    "    dM3 = dH2\n",
    "\n",
    "    # MLP back\n",
    "    dW2 = (M2.reshape(-1, d_ff).T @ dM3.reshape(-1, d))\n",
    "    dM2 = dM3 @ W2.T\n",
    "    dM1 = dM2 * dgelu(M1)\n",
    "    dW1 = (H1n.reshape(-1, d).T @ dM1.reshape(-1, d_ff))\n",
    "    dH1n = dM1 @ W1.T\n",
    "\n",
    "    # RMSNorm back\n",
    "    dH1_rms, dg_mlp_local = rmsnorm_bwd(dH1n, cache_rms)\n",
    "    d_g_mlp = dg_mlp_local\n",
    "\n",
    "    dH1 += dH1_rms  # residual path added above MLP\n",
    "\n",
    "    # Attention back: H_attn = (A @ Vv) @ W_O\n",
    "    dH_attn = dH1\n",
    "    dW_O = (O.reshape(-1, d).T @ dH_attn.reshape(-1, d))\n",
    "    dO = dH_attn @ W_O.T\n",
    "\n",
    "    dA = dO @ Vv.transpose(0,2,1)        # (B,T,T)\n",
    "    dVv = A.transpose(0,2,1) @ dO        # (B,T,d)\n",
    "\n",
    "    # softmax backward: for each row, J = diag(p) - p p^T\n",
    "    # Efficient: dS = dA - sum(dA* A, axis=-1, keepdims=True) * A\n",
    "    sum_dA_A = np.sum(dA * A, axis=-1, keepdims=True)\n",
    "    dS = dA - sum_dA_A * A\n",
    "\n",
    "    # causal mask blocks grad\n",
    "    dS = np.where(mask[None,:,:], 0.0, dS)\n",
    "\n",
    "    # S = (Q K^T)/sqrt(d)\n",
    "    dQ = (dS @ K) / np.sqrt(d)          # (B,T,d)\n",
    "    dK = (dS.transpose(0,2,1) @ Q) / np.sqrt(d)\n",
    "\n",
    "    # Q = H0 W_Q, etc.\n",
    "    dW_Q = (H0.reshape(-1, d).T @ dQ.reshape(-1, d))\n",
    "    dW_K = (H0.reshape(-1, d).T @ dK.reshape(-1, d))\n",
    "    dW_V = (H0.reshape(-1, d).T @ dVv.reshape(-1, d))\n",
    "\n",
    "    dH0 = dQ @ W_Q.T\n",
    "    dH0 += dK @ W_K.T\n",
    "    dH0 += dVv @ W_V.T\n",
    "\n",
    "    # Residual from H1 = H0 + H_attn\n",
    "    dH0 += dH1\n",
    "\n",
    "    # Back to embeddings/positions\n",
    "    dP = np.sum(dH0, axis=0)  # (T,d)\n",
    "    dX = dH0                  # (B,T,d)\n",
    "\n",
    "    # E lookup scatter‑add\n",
    "    dE = np.zeros_like(E)\n",
    "    # add dX to rows selected by x\n",
    "    for b in range(B):\n",
    "        np.add.at(dE, x[b], dX[b])\n",
    "\n",
    "    grads = {\n",
    "        \"E\": dE, \"P\": dP,\n",
    "        \"W_Q\": dW_Q, \"W_K\": dW_K, \"W_V\": dW_V, \"W_O\": dW_O,\n",
    "        \"g_mlp\": d_g_mlp,\n",
    "        \"W1\": dW1, \"W2\": dW2,\n",
    "        \"WLM\": dWLM\n",
    "    }\n",
    "\n",
    "    cache_vals = {\n",
    "        \"loss\": loss\n",
    "    }\n",
    "    return loss, grads\n",
    "\n",
    "# -----------------------\n",
    "# Update (SGD)\n",
    "# -----------------------\n",
    "def sgd(param, grad, lr):\n",
    "    param -= lr * grad\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "def train(steps=500, bs=16, print_every=50):\n",
    "    global E,P,W_Q,W_K,W_V,W_O,g_mlp,W1,W2,WLM\n",
    "    for step in range(1, steps+1):\n",
    "        x, y = get_batch(bs)\n",
    "        loss, g = forward_backward(x, y)\n",
    "\n",
    "        # update\n",
    "        sgd(E,   g[\"E\"], lr)\n",
    "        sgd(P,   g[\"P\"], lr)\n",
    "        sgd(W_Q, g[\"W_Q\"], lr)\n",
    "        sgd(W_K, g[\"W_K\"], lr)\n",
    "        sgd(W_V, g[\"W_V\"], lr)\n",
    "        sgd(W_O, g[\"W_O\"], lr)\n",
    "        sgd(g_mlp, g[\"g_mlp\"], lr)\n",
    "        sgd(W1,  g[\"W1\"], lr)\n",
    "        sgd(W2,  g[\"W2\"], lr)\n",
    "        sgd(WLM, g[\"WLM\"], lr)\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            print(f\"step {step}: loss {loss:.4f}\")\n",
    "\n",
    "def sample(prefix=\"tiny \", max_new=200, temperature=1.0, top_k=None):\n",
    "    ctx = encode(prefix)\n",
    "    ctx = ctx[-T:]  # clip\n",
    "    for _ in range(max_new):\n",
    "        # single forward (no grad) on last T tokens\n",
    "        x = np.zeros((1,T), dtype=np.int64)\n",
    "        # left-pad with last tokens\n",
    "        if len(ctx) < T:\n",
    "            x[0, :len(ctx)] = ctx\n",
    "        else:\n",
    "            x[0] = ctx[-T:]\n",
    "\n",
    "        # Forward until logits (reusing forward code without storing grads)\n",
    "        X = E[x] + P[None,:,:]\n",
    "        Q = (X @ W_Q); K=(X @ W_K); Vv=(X @ W_V)\n",
    "        S = (Q @ K.transpose(0,2,1))/np.sqrt(d)\n",
    "        S = np.where(mask[None,:,:], -1e9, S)\n",
    "        A = softmax(S)\n",
    "        O = A @ Vv\n",
    "        H1 = X + (O @ W_O)\n",
    "        # pre-norm MLP\n",
    "        H1n, _ = rmsnorm_fwd(H1, g_mlp)\n",
    "        H2 = H1 + gelu(H1n @ W1) @ W2\n",
    "        logits = H2 @ WLM\n",
    "        last = logits[0, len(ctx)-1 if len(ctx)<=T else T-1]  # last step\n",
    "\n",
    "        # temperature\n",
    "        logits_t = last / max(1e-8, temperature)\n",
    "\n",
    "        # top-k\n",
    "        if top_k is not None:\n",
    "            idx = np.argpartition(logits_t, -top_k)[-top_k:]\n",
    "            mask_k = np.ones_like(logits_t, dtype=bool)\n",
    "            mask_k[idx] = False\n",
    "            logits_t[mask_k] = -1e9\n",
    "\n",
    "        probs = softmax(logits_t[None,:])[0]\n",
    "        next_id = np.random.choice(len(probs), p=probs)\n",
    "        ctx = np.concatenate([ctx, [next_id]])\n",
    "        if len(ctx) > 4*T:  # keep it bounded\n",
    "            ctx = ctx[-T:]\n",
    "    return decode(ctx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Vocab size:\", V, \"Unique chars:\", \"\".join(chars))\n",
    "    train(steps=400, bs=16, print_every=50)\n",
    "    print(sample(prefix=\"tiny \", max_new=200, temperature=0.9, top_k=20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
