{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135c3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e032da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6c986",
   "metadata": {},
   "source": [
    "I give the probability at result of softmax will be:\n",
    "$$softmax(logits) = p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69295ee4",
   "metadata": {},
   "source": [
    "for example I give them value of p in case of we have only 6 word in total vocab:\n",
    "    $$ p =\\begin{bmatrix} 0.1 & 0.2 &0.4 &0.1 &0.0 &0.2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82e5eb",
   "metadata": {},
   "source": [
    "and we have the exact result is y :\n",
    "$$y = \\begin{bmatrix} 0 & 0 & 1 & 0 & 0 & 0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4c1ed",
   "metadata": {},
   "source": [
    "because y is some constant :\n",
    "$$changing\\_weight\\_q\\_value\\_by =  -\\frac {\\partial\\left(p \\right)}{\\partial q}$$\n",
    "$$changing\\_weight\\_q\\_value\\_by =  -\\frac {\\partial\\left(softmax(logits) \\right)}{\\partial logits} \\cdot \\frac {\\partial\\left(logits \\right)}{\\partial q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d24c1",
   "metadata": {},
   "source": [
    "from definition of error it should be like:\n",
    "$$Error = {\\left(y - p\\right)}^2$$\n",
    "\n",
    "but we use this function for calculate error instead:\n",
    "$$\\mathcal{L} = - \\sum_{i = 0}^{V} y_i \\log_e \\left(p_i\\right)$$\n",
    "\n",
    "call **\"cross-entropy loss\"**\n",
    "\n",
    "from the value of y จะได้ว่าค่า $\\mathcal{L}$ จะมีค่าเป็น 0 หาก $y = 0$\n",
    "และ $$\\frac{\\partial\\mathcal{L}}{\\partial q} = 0 ; y_i = 0$$ ด้วย"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea089794",
   "metadata": {},
   "source": [
    "from $$\\frac{d}{dx} \\log_a(x) = \\frac {1} {x \\log_e {a}} = \\frac {1} {x \\ln\\left(a\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b804c9",
   "metadata": {},
   "source": [
    "if we want to decrease error by change some value that will be combine into p \n",
    "we will write it to :\n",
    "$$ changing\\_weight\\_q\\_value\\_by = \\frac {\\partial\\mathcal{L}}{\\partial q}$$\n",
    "$$ \\frac {\\partial\\mathcal{L}}{\\partial q} = \\frac {\\partial}{\\partial q} \\left(-\\sum_{i = 0}^V \\left( y_i \\ln \\left(p_i\\right) \\right)\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ac5c0",
   "metadata": {},
   "source": [
    "from the y array we found that the equation have the value only at $y_i = 1$ then we got \n",
    "$$ \\frac {\\partial\\mathcal{L}}{\\partial q} = -\\sum_{i=0}^V\\frac {\\partial\\left( y_i \\ln(p_i)\\right)}{\\partial p_i} \\cdot \\frac{\\partial p_i} {\\partial q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cb887",
   "metadata": {},
   "source": [
    "$$ \\frac {\\partial\\mathcal{L}}{\\partial q} = -\\frac {y_i}{p_i \\ln\\left(e\\right)} \\cdot \\frac{\\partial p_i} {\\partial q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3679c55",
   "metadata": {},
   "source": [
    "## Softmax derivation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cb0f7",
   "metadata": {},
   "source": [
    "from softmax equation:\n",
    "$$p_i = softmax(z_i) = \\frac{e^{z_i}}{\\sum_{k = 0}^{V - 1} \\left(e^{z_k}\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f28281",
   "metadata": {},
   "source": [
    "**การหาอนุพันธ์ของ Softmax จะพิเศษกว่าขั้นตอนอื่นเนื่องจาก $softmax\\left(z_i\\right)$ ไม่ได้ขึ้นอยู่กับแค่ $z_i$ แต่จะขึ้นอยู่กับค่าของ logits ทุกตัว\n",
    "ดังนั้น**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd706a1",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "if $$logits = \\begin{bmatrix}z_0&z_1&...&z_j&...&z_{V - 1}\\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial p_i} {\\partial z_m} = ?$$\n",
    "if $m = \\begin{matrix} 0, 1,..., V-1 \\end{matrix}$\n",
    "\n",
    "We start with :\n",
    "\n",
    "$$\\frac{d}{dz_m} \\frac{U}{S} = \\frac{U^\\prime S - US^\\prime}{S^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53af2c",
   "metadata": {},
   "source": [
    "\n",
    "$$U = e^{z_i}$$\n",
    "and \n",
    "$$S = \\sum_{k=0}^S \\left(e^{z_k}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e771a88",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial U} {\\partial z_m} = \n",
    "\\begin{cases} \n",
    "    e^{z_i} & \\text{if $i = m$}\\\\ \n",
    "    0 & \\text {if $i \\neq m$} \n",
    "\\end{cases}$$\n",
    "**Or**\n",
    "$$\\frac {\\partial U} {\\partial z_m} = e^{z_i}\\delta_{i,m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9eb35",
   "metadata": {},
   "source": [
    "then in case of $i = m$ we got : \n",
    "\n",
    "$$\\frac{\\partial p_i}{\\partial z_m} = \\frac{e^{z_i} S - e^{2\\cdot z_i}}{S^2}$$\n",
    "$$\\frac{\\partial p_i}{\\partial z_m} = \\frac{e^{z_i}}{S} \\cdot \\frac{S - e^{z_i}}{S}$$\n",
    "\n",
    "finally got\n",
    "$$\\frac{\\partial p_i}{\\partial z_m} = p_i \\cdot \\left(1 - p_i\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2e3ed",
   "metadata": {},
   "source": [
    "if $i \\neq m$ :\n",
    "$$\n",
    "    \\frac{\\partial p_i}{\\partial z_m} = \\frac{0 \\cdot S - e^{z_i}e^{z_m}}{V^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial p_i}{\\partial z_m} = -\\frac{e^{z_i}e^{z_m}}{S^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b89868",
   "metadata": {},
   "source": [
    "**Or**\n",
    "$$\\frac{\\partial p_i}{\\partial z_m} = p_i \\cdot \\left(\\delta_{i,m} - p_i\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a39cb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b081ca3",
   "metadata": {},
   "source": [
    "## Mixing up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0de248",
   "metadata": {},
   "source": [
    "We have\n",
    "$$\\frac{\\partial p_i}{\\partial z_m} = p_i \\cdot \\left(\\delta_{i, m} - p_j\\right)$$\n",
    "และ loss function (cross-entropy) ต่อ 1 ตำแหน่ง:\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_{i = 0}^{V} y_i \\log_e \\left(p_i\\right)$$\n",
    "\n",
    "โดย y มักจะเป็น one-hot (แต่ derivation นี้ใช้ได้กับ distribution ด้วย)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f1187",
   "metadata": {},
   "source": [
    "#### 1.) เริ่ม chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e69a65",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial \\mathcal {L}} {\\partial z_m} =\n",
    "\\sum_{j=0}^{V - 1} \\frac{\\partial \\mathcal{L}}{\\partial p_i} \\cdot\n",
    "\\frac {\\partial p_i}{\\partial z_m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba9469",
   "metadata": {},
   "source": [
    "หา $\\frac {\\partial \\mathcal{L}}{\\partial p_i}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8566ee",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial \\mathcal{L}}{\\partial p_i} = -\\frac{y_i}{p_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2312882",
   "metadata": {},
   "source": [
    "แทนกลับเข้าไป :\n",
    "$$\\frac {\\partial \\mathcal {L}} {\\partial z_m} =\n",
    "\\sum_{i=0}^{V - 1} \\left(\\left(-\\frac{y_i}{p_i} \\right)\\cdot\n",
    "p_i \\cdot \\left(\\delta_{i, m} - p_i\\right)\\right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59669cb9",
   "metadata": {},
   "source": [
    "ตัด $p_i$ ทิ้ง :\n",
    "$$\\frac {\\partial \\mathcal {L}} {\\partial z_m} =\n",
    "\\sum_{j=0}^{V - 1} \\left(-{y_i}  \\cdot \\left(\\delta_{i, m} - p_i\\right)\\right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e463e7",
   "metadata": {},
   "source": [
    "#### 2.) แยกผลรวมออกเป็น 2 ก้อน (นี่คือจุดที่ \"เคสอื่น ๆ\" จะถูกกินรวม)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b876f6",
   "metadata": {},
   "source": [
    "$$-\\sum_i y_i\\left(\\delta_{i, m} - p_m\\right) = - \\sum_i y_i \\delta_{i, m} + \\sum_i y_i p_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df105b0",
   "metadata": {},
   "source": [
    "สังเกตว่า $p_m$ ไม่ขึ้นกับ $i$ (สำหรับการ sum ตาม $i$) จึงสามารถดึงออกมาจากผลรวมได้:\n",
    "\n",
    "$$ \\frac {\\partial \\mathcal{L}} {\\partial z_m} = -\\sum_i y_i \\delta_{i, m} + p_m \\sum_i y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f39dc",
   "metadata": {},
   "source": [
    "#### 3.) ประเมินผลรวมของ $\\sum_i y_i \\delta_{i,m}$ และ $\\sum_i y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffd49d",
   "metadata": {},
   "source": [
    "* $\\sum_i y_i \\delta_{i, m}$ : $\\delta_{i, m}$ ทำหน้าที่ \"เลือก\" เฉพาะตำแหน่ง $i = m$ ดังนั้น : \n",
    "$$ \\sum_i y_i \\delta_{i, m} = y_m$$\n",
    "\n",
    "* $\\sum_i y_i$ : ไม่ว่า $y$ เป็น one-hot หรือจะเป็น probability distribution จะมีค่าผลรวมเท่ากับ 1 เสมอ ดังนั้น :\n",
    "$$ \\sum_i y_i = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7de06f",
   "metadata": {},
   "source": [
    "#### 4.) สรุปออกมาเป็น $p-y$\n",
    "\n",
    "แทนค่า : \n",
    "$$\\frac{\\partial \\mathcal {L}} {\\partial z_m} = -y_m + p_m \\cdot 1 = p_m - y_m$$\n",
    "\n",
    "ดังนั้นในรูป vector : \n",
    "$$\\frac{\\partial \\mathcal {L}} {\\partial \\vec{z}} = \\vec{p}- \\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cac135",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e50c7",
   "metadata": {},
   "source": [
    "## Learn from dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bf174",
   "metadata": {},
   "source": [
    "สมมุติในการเทรน LLM เราจะมีข้อความหนึ่งชุด แล้วตัดเป็นชิ้นยาว T = 8 tokens:\n",
    "* ถ้าใน 1 step เราใช้ข้อความแค่ชิ้นเดียว -> Batch size $B = 1$\n",
    "    * input shape: (1, 8)\n",
    "    * model จะทำนาย next-token 8 ตำแหน่ง\n",
    "    * จำนวน token ที่ฝึกต่อ step = $B \\cdot T = 8$ \n",
    "\n",
    "ตัวอย่าง batch จริง ๆ (หลายชิ้นพร้อมกัน)\n",
    "\n",
    "สมมติเราเตรียมชิ้นข้อความ (แต่ละชิ้นยาว 8 tokens) ได้ 4 ชิ้น : \n",
    "* Step หนึ่งใช้ 4 ชิ้นพร้อมกัน -> Batch size $B = 4$\n",
    "    * input shape: (4, 8)\n",
    "    * จำนวน token ที่ฝึกต่อ step = $4 \\cdot 8 = 32$\n",
    "\n",
    "**ผลที่ได้คือ** : gradient ที่ได้ต่อ step จะมีความนิ่งขึ้น เพราะมาจากตัวอย่างที่มากขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca7a8b",
   "metadata": {},
   "source": [
    "\n",
    "* ตอนที่เราทำการเทรนทีละ Sequence หรือ ฺ$B = 1$ ใช้สมการเดิมได้ไม่มีปัญหา\n",
    "\n",
    "* แต่ถ้าเอา 32 sequence มารวมกันเป็น batch -> $B = 32$\n",
    "\n",
    "ทำให้ shape ที่ได้เป็น : \n",
    "\n",
    "* logits $Z: (B, T, V)$\n",
    "* probs $P: (B, T, V)$\n",
    "* targets $Y: (B, T, V)$ (one-hot) หรือ (B, T)(index)\n",
    "\n",
    "---\n",
    "\n",
    "ทำให้ตอนคิด Loss function จะต้องคิดเป็นค่าเฉลี่ย (mean) (เป็นที่นิยมใช้) แล้วได้สมการเป็น\n",
    "\n",
    "$$\\mathcal {L} = \\frac {1} {B \\cdot T} \\sum_{b=1}^B \\sum_{t=1}^T \\left(-ln \\left(p_{b, t, y_{b,t}}\\right)\\right)$$\n",
    "\n",
    "แล้วทำให้ gradient มี factor $\\frac {1} {B \\cdot T}$ ติดมาด้วย : \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}} {\\partial \\vec{Z}} = \\frac{\\vec{P} - \\vec{Y}}{B \\cdot T} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f33ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis = -1):\n",
    "\n",
    "    # หาค่ามากสุดตามแนวแกนที่เลือกมาแล้วหักลบกับแต่ละค่าในแมทริกส์\n",
    "    # axis = -1 : [1, 2, 3], [4, 5, 6] -> (2, 3) shape \n",
    "    # ได้คำตอบเป็น [[3], [6]] ที่เป็นค่ามากสุดแต่ละค่าตามแนวแกน X\n",
    "    x = x - np.max(x, axis = axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e, axis = axis, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0edea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5cbcedb",
   "metadata": {},
   "source": [
    "### 1) for example Vocab มี 7 ตัว\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2754bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = [\"<PAD>\", \"I\", \"You\", \"love\", \"a\", \"cat\", \"dog\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a2774",
   "metadata": {},
   "source": [
    "มีทั้งหมด 2 ประโยค (Batch size $B = 2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceA = \"I love a cat\"\n",
    "sentenceB = \"You love a dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf20c2",
   "metadata": {},
   "source": [
    "เราจะตั้ง sequence length ที่ใช้เทรนเป็น $T = 3$ (เอาแค่ 3 ตัวแรกเป็น input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df11206",
   "metadata": {},
   "source": [
    "### 2 สร้าง X (input) และ Y (output) ด้วยการเลื่อนไปทางซ้าย 1 ระดับ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb05416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = [1, 3, 4]\n",
    "Y_a = [3, 4, 5]\n",
    "\n",
    "X_b = [2, 3, 4]\n",
    "Y_b = [3, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c73fe",
   "metadata": {},
   "source": [
    "### 3) เขียนเป็น Matrix (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49a6b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([X_a, X_b]);\n",
    "Y = np.array([Y_a, Y_b]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f76be",
   "metadata": {},
   "source": [
    "### 4) Model ให้ logits ออกมาเป็น (B, T, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d921df7",
   "metadata": {},
   "source": [
    "$Z$ หรือ logits หมายถึงคะแนนดิบก่อน softmax\n",
    "$$ Z \\in \\mathbb{R}^{2 \\times 3 \\times 7}$$\n",
    "ในกรณีนี้\n",
    "* มี ฺBatch size = 2 \n",
    "* มี token size = 3 \n",
    "* มี Vocab size (V) = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dda70d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_0 = np.array([\n",
    "    [-1, .2, -.3, 2.5, .1, -.7, 0.1],\n",
    "    [-.4, -.2, -.5, .1, 2.2, 0, 0.1],\n",
    "    [-.8, -.3, -.2, 0, .4, 2.6, 0.1]\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c02b06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_1 = np.array([\n",
    "    [-1.1, -.1, 0, 2.3, .2, -.6, .1], \n",
    "    [-.5, -.3, -.4, .2, 2.4, -.1, .1],\n",
    "    [-.9, -.4, -.1, .1, .3,.1, 2.5]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6b9cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 7)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.array([Z_0, Z_1])\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586fff32",
   "metadata": {},
   "source": [
    "### 5) softmax ให้ได้รูปแบบ $P (B, T, V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a91e9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.02136409 0.07093128 0.043022   0.70748156 0.06418128 0.02883851\n",
      "   0.06418128]\n",
      "  [0.04677434 0.0571303  0.04232317 0.07711784 0.6297574  0.06977911\n",
      "   0.07711784]\n",
      "  [0.02356257 0.0388481  0.04293379 0.05243945 0.07823047 0.70603106\n",
      "   0.05795456]]\n",
      "\n",
      " [[0.02206303 0.05997354 0.06628102 0.66109895 0.08095582 0.03637579\n",
      "   0.07325185]\n",
      "  [0.03727391 0.04552646 0.04119404 0.07506044 0.6774215  0.05560614\n",
      "   0.0679175 ]\n",
      "  [0.02293839 0.03781901 0.05105032 0.062353   0.07615813 0.062353\n",
      "   0.68732815]]]\n"
     ]
    }
   ],
   "source": [
    "P = softmax(Z)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b900e2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 7)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886e3ad",
   "metadata": {},
   "source": [
    "**Structure of P และทำไมถึงใช้วิธีการหยิบค่ามาคำนวณได้เลย**\n",
    "\n",
    "current output of P consist of 2 batch (index = 0 of shape)\n",
    "ความน่าจะเป็นของโทเคนที่จะออกถัดไปของแต่ละ x ใน index = 1 of shape \n",
    "* ยกตัวอย่างของ batch ที่ 1 (P[0]) เราจะได้ว่าผลการเดาโทเคนตัวต่อไปของคำว่า I คือ (love, มีค่าความน่าจะเป็น P[0, 0, 3] = 0.75600279) \n",
    "    - แต่จาก one hot Y vector คำตอบที่ถูกต้องจะเป็น $Y = \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 & 0 \\end{bmatrix}$\n",
    "    - จากสมการเต็ม\n",
    "    $$ \\mathcal{L} = \\frac{1} {B \\cdot T} \\sum_{b=0}^{B-1} \\sum_{t=0}^{T-1} y_t \\ln \\left(p_t\\right)$$\n",
    "    - แต่ที่ $Y_{t \\neq ตำแหน่งคำตอบที่ถูกต้อง} = 0$ จึงทำให้สมการลดรูปเหลือ\n",
    "    $$ \\mathcal{L} = \\frac{1} {B \\cdot T} \\sum_{b, t} - \\ln \\left(P[b, t, Y[b, T]]\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a4db",
   "metadata": {},
   "source": [
    "### 6) คิด loss ด้วย Y (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead269a",
   "metadata": {},
   "source": [
    "เพราะ Y เก็บเป็น id ไม่ใช่ one-hot ทำให้เราคิด cross-entropy แบบหยิบเฉพาะคำ\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1} {B \\cdot T} \\sum_{b, t} -ln \\left(P[b, t, Y[b, t]]\\right)$$\n",
    "\n",
    "ตัวอย่างการหยิบค่ามาคิด loss\n",
    "\n",
    "* สำหรับ A ที่ $t=0$ : target คือ $Y[0, 0] = 3$ (love) -> ใช้ค่า $P[0, 0, 3]$\n",
    "* สำหรับ A ที่ $t=1$ : target คือ $Y[0, 1] = 4$ (a) -> ใช้ค่า $P[0, 1, 4]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60535a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.7074815607441387\n"
     ]
    }
   ],
   "source": [
    "print(Y[0, 0])\n",
    "print(P[0, 0, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edd4ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ของรอบนี้คือ  0.3891361962995475\n"
     ]
    }
   ],
   "source": [
    "B = P.shape[0] #total batch\n",
    "T = P.shape[1] #total token in each step\n",
    "\n",
    "totalLoss = 0\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        totalLoss += - np.log(P[b, t, Y[b, t]])\n",
    "Loss = totalLoss / (B * T)\n",
    "\n",
    "print(\"Loss ของรอบนี้คือ \", Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc87bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5313d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5\n",
    "T = 6\n",
    "V = 20\n",
    "\n",
    "p = np.array([\n",
    "    [.1, .2, .4, .1, 0, .1, 0], \n",
    "    [.1, .1, .2, 0, 5, .1, 0]]);\n",
    "y = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0, 1]\n",
    "    ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa7a58",
   "metadata": {},
   "source": [
    "$$\\text{softmax}\\left(\\text{logits}\\right) =  \\frac{e^{logits_i-logits_{max}}}{\\sum_{i = 0}^{V-1} e^{logits_i-logits_{max}}}$$\n",
    "\n",
    "When $V$ is vocab size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d0b9a",
   "metadata": {},
   "source": [
    "### 7) แล้ว Gradient of logits จะเขียนได้ดังนี้"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24816eca",
   "metadata": {},
   "source": [
    "$$ \\frac {\\partial \\mathcal {L}} {\\partial Z} = \\frac {P - Y_{onehot}} {B \\cdot T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb25a6",
   "metadata": {},
   "source": [
    "วิธีด้านล่างนี้ผิด ==>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7670cc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of logits ในการคำนวณรอบนี้คือ  -3.488480230664544\n"
     ]
    }
   ],
   "source": [
    "#find gradient of logits\n",
    "B = P.shape[0]\n",
    "T = P.shape[1]\n",
    "\n",
    "\n",
    "# วิธีนี้ผิด\n",
    "totalGradientOfLogits = 0\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        totalGradientOfLogits += P[b, t, Y[b, t]] - Y[b,t]\n",
    "\n",
    "GradientOfLogits = totalGradientOfLogits / (B * T)\n",
    "print (\"Gradient of logits ในการคำนวณรอบนี้คือ \", GradientOfLogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d75a0",
   "metadata": {},
   "source": [
    "#### วิธีที่ถูกต้อง ==>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95afd80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ได้ Onehot of Y เป็น\n",
      "[[[0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# แบบใช้ one hot vector Y\n",
    "\n",
    "B, T = Y.shape\n",
    "V = P.shape[2]\n",
    "\n",
    "Y_onehot = np.zeros((B, T, V), dtype=np.float64)\n",
    "Y_onehot[np.arange(B)[:, None], np.arange(T)[None, :], Y] = 1.0\n",
    "\n",
    "print(\"ได้ Onehot of Y เป็น\")\n",
    "print(Y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0499a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แล้วได้ Gradient of logits เป็น \n",
      "[[[ 0.00356068  0.01182188  0.00717033 -0.04875307  0.01069688\n",
      "    0.00480642  0.01069688]\n",
      "  [ 0.00779572  0.00952172  0.00705386  0.01285297 -0.0617071\n",
      "    0.01162985  0.01285297]\n",
      "  [ 0.00392709  0.00647468  0.00715563  0.00873991  0.01303841\n",
      "   -0.04899482  0.00965909]]\n",
      "\n",
      " [[ 0.00367717  0.00999559  0.01104684 -0.05648351  0.01349264\n",
      "    0.00606263  0.01220864]\n",
      "  [ 0.00621232  0.00758774  0.00686567  0.01251007 -0.05376308\n",
      "    0.00926769  0.01131958]\n",
      "  [ 0.00382306  0.00630317  0.00850839  0.01039217  0.01269302\n",
      "    0.01039217 -0.05211198]]]\n"
     ]
    }
   ],
   "source": [
    "GradientOfLogits = (P - Y_onehot) / (B * T)\n",
    "\n",
    "print(\"แล้วได้ Gradient of logits เป็น \")\n",
    "print(GradientOfLogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00070228",
   "metadata": {},
   "source": [
    "จะเห็นได้ว่า gradient ของ lost function และ softmax (gradient ของ logits) $\\frac {\\partial \\mathcal {L} } {\\partial P} \\cdot \\frac {\\partial P} {\\partial Z}$ รวมกันมีมิติ เท่ากับ logits\n",
    "\n",
    "$$\\frac {\\partial \\mathcal {L}} {\\partial Z} \\in \\mathbb {R} ^ {B \\times T \\times V}$$\n",
    "\n",
    "Where \n",
    "* $B$ is batch size\n",
    "* $T$ is token size\n",
    "* $V$ is vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac1afd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 7)\n",
      "[[[ 0.00356068  0.01182188  0.00717033 -0.04875307  0.01069688\n",
      "    0.00480642  0.01069688]\n",
      "  [ 0.00779572  0.00952172  0.00705386  0.01285297 -0.0617071\n",
      "    0.01162985  0.01285297]\n",
      "  [ 0.00392709  0.00647468  0.00715563  0.00873991  0.01303841\n",
      "   -0.04899482  0.00965909]]\n",
      "\n",
      " [[ 0.00367717  0.00999559  0.01104684 -0.05648351  0.01349264\n",
      "    0.00606263  0.01220864]\n",
      "  [ 0.00621232  0.00758774  0.00686567  0.01251007 -0.05376308\n",
      "    0.00926769  0.01131958]\n",
      "  [ 0.00382306  0.00630317  0.00850839  0.01039217  0.01269302\n",
      "    0.01039217 -0.05211198]]]\n"
     ]
    }
   ],
   "source": [
    "# index target method\n",
    "\n",
    "B, T, V = P.shape\n",
    "\n",
    "dZ = P.copy()\n",
    "dZ /= (B * T)\n",
    "\n",
    "# subtract 1/(B * T) at the true class indices\n",
    "\n",
    "dZ[np.arange(B)[:, None], np.arange(T)[None, :], Y] -= 1.0 / (B * T)\n",
    "\n",
    "print (dZ.shape)\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d062631",
   "metadata": {},
   "source": [
    "## Padding and loss mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d375e",
   "metadata": {},
   "source": [
    "Loss mark คือ ตัวกรองว่า ตำแหน่งไหน ใน Sequence ที่เราจะนำไปคิด Loss และตำแหน่งไหน ไม่คิด  \n",
    "\n",
    "**โดยจะใช้ตอนคำนวณ Loss เท่านั้นไม่คิดใน attention**\n",
    "\n",
    "**Loss mark ใช้ทำอะไรบ้าง**\n",
    "\n",
    "1. กัน PAD ไม่ให้ถูกเอาไปคิด loss\n",
    "\n",
    "เวลา batch ประโยคยาวไม่เท่ากัน เราต้อง pad ให้ยาวเท่ากันเช่นการใส่ <PAD> = id 0 ตำแหน่งที่ PAD ไม้ใช่ข้อมูลจริง -> *ไม่ควรคิด loss*\n",
    "\n",
    "2. Instruction tuning / Chat format\n",
    "\n",
    "เรามัก ไม่อยากให้โมเดลเรียนรู้การทวน Prompt เราอยากให้เรียนเฉพาะคำตอบ (assistanct)\n",
    "เลย mark ตำแหน่ง prompt = 0 และคำตอบ = 1\n",
    "\n",
    "3. Ignore special tokens\n",
    "\n",
    "เช่น <BOS> <SEP> หรือบางคำที่ไม่อยากให้ train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d15f4",
   "metadata": {},
   "source": [
    "### Shape of loss mark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985bb9a",
   "metadata": {},
   "source": [
    "loss mark มักจะมี shape เดียวกับ target tokens:\n",
    "* $Y \\in \\mathbb {R} ^ {B \\times T}$\n",
    "* $loss\\_mark \\in \\left\\{1, 0\\right\\} ^ {B \\times T}$ และมีค่าข้างในแค่ 0 และ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d67ca1",
   "metadata": {},
   "source": [
    "### สูตรคำนวณ loss แบบมี mask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5aefc",
   "metadata": {},
   "source": [
    "$$\\mathcal {L} = \\frac {\\sum_{b,t} m_{b, t} \\cdot \\left(-log \\left( P [b, t, Y[b, t]]\\right)\\right)} {\\sum_{b, t} m_{b, t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388e345",
   "metadata": {},
   "source": [
    "### 1) Demo data\n",
    "---\n",
    "* token ที่เป็น system / user / prompt &rarr; mask = 0\n",
    "* token ที่เป็น assistant response &rarr; mask = 1\n",
    "\n",
    "ในขั้นตอนของการ tokenization ถ้ารู้ช่วง index ของ assistant เช่นช่วง $\\left[ start : end \\right)$ สามารถใช้คำสั่ง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c95748b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#focus ที่ mask y\n",
    "\n",
    "start = 0;\n",
    "end = 2\n",
    "loss_mask = np.zeros((B, T), dtype=np.float64)\n",
    "loss_mask[b, start: end] = 1\n",
    "\n",
    "print(loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8db9d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "T = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04be7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mask1 = [1, 3, 4, 5] #I love a cat\n",
    "Y_mask1 = [3, 4, 5, 0] #love a cat <PAD>\n",
    "\n",
    "X_mask2 = [3, 4, 5, 0] #love a cat <PAD>\n",
    "Y_mask2 = [4, 5, 0, 0] #cat <PAD> <PAD>\n",
    "\n",
    "pad_id = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b676449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 5 0]\n",
      " [4 5 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([Y_mask1, Y_mask2]);\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9784a36",
   "metadata": {},
   "source": [
    "loss_mask (1 เฉพาะตำแหน่งที่ไม่ใช่ PAD)\n",
    "\n",
    "> * m คือ loss mask matrix\n",
    "> * M คือ จำนวน token ทั้งหมดที่จะถูกนำไปเทรน"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68e0a2",
   "metadata": {},
   "source": [
    "กติกา: ตำแหน่ง Y == pad_id -> mask = 0 (ไม่คิด loss)\n",
    "ตำแหน่งที่ Y != pad_id -> mask = 1 (คิด loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7233eb",
   "metadata": {},
   "source": [
    "จำนวนตำแหน่งที่คิด loss ทั้งหมด\n",
    "$$M = \\sum loss\\_mask = 4$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "002ff08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 0.]\n",
      " [1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ถ้าค่าใน Y ไม่เท่ากับ pad_id (0) จะได้เป็นค่า 1 นอกนั้น 0\n",
    "m = (Y != pad_id).astype(np.float64) # (B, T)\n",
    "M = m.sum()\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e73f8",
   "metadata": {},
   "source": [
    "สมมุติ logits $ Z \\in \\mathbb {R} ^ {B \\times T \\times V}$ \n",
    "\n",
    "**ปรกติเลขที่ได้จะไม่ใช่ 0 ล้วนแต่เพื่อให้ง่ายต่อการมองเห็นภาพ โดยปรกติแล้วตำแหน่งของเลขที่มากที่สุดจะหมายถึง ความน่าจะเป็นที่ token ถัดไปจะออก**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f0fd5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.  0.  0.  3.  4.  0.  0. ]\n",
      "  [0.  0.  0.  0.  1.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  2.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.5 0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0.  0.  0.  0. ]]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros((B, T, V), dtype=np.float64)\n",
    " \n",
    "Z[0, 0, 3] = 3.0\n",
    "Z[0, 1, 4] = 1.0 # (b0, t1) target 4\n",
    "Z[0, 2, 5] = 2.0 # (b0, t2) target 5\n",
    "Z[0, 0, 4] = 4\n",
    "Z[1, 1, 5] = 0.5 # (b1, t1) target 5\n",
    "\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bafb6",
   "metadata": {},
   "source": [
    "นำค่า logits ที่ได้ไปหาค่าความน่าจะเป็น $P$ โดยใช้ softmax function &rarr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c447c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.01254962 0.01254962 0.01254962 0.25206586 0.68518604 0.01254962\n",
      "   0.01254962]\n",
      "  [0.1147015  0.1147015  0.1147015  0.1147015  0.311791   0.1147015\n",
      "   0.1147015 ]\n",
      "  [0.07468786 0.07468786 0.07468786 0.07468786 0.07468786 0.55187282\n",
      "   0.07468786]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]\n",
      "\n",
      " [[0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.13074081 0.13074081 0.13074081 0.13074081 0.13074081 0.21555515\n",
      "   0.13074081]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]]\n"
     ]
    }
   ],
   "source": [
    "P = softmax(Z)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834979a2",
   "metadata": {},
   "source": [
    "one-hot matrix Y &rarr; Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58d178c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "Y_onehot = np.zeros((B, T, V), dtype=np.float64)\n",
    "Y_onehot[np.arange(B)[:, None], np.arange(T)[None, :], Y] = 1.0\n",
    "\n",
    "print(Y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85213a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', 'I', 'You', 'love', 'a', 'cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabc371",
   "metadata": {},
   "source": [
    "[[[0. 0. 0. 0. 1. 0.] >> คำต่อไปที่จะออกคือ \"a\"  \n",
    "  [0. 0. 0. 0. 0. 1.] >> คำต่อไปที่จะออกคือ \"cat\"  \n",
    "  [1. 0. 0. 0. 0. 0.]] >> คำต่อไปที่จะออกคือ \"PAD\"  \n",
    "\n",
    " [[0. 0. 0. 0. 0. 1.]  \n",
    "  [1. 0. 0. 0. 0. 0.]  \n",
    "  [1. 0. 0. 0. 0. 0.]]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97bda337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_correct ->\n",
      " [[0.25206586 0.311791   0.55187282 0.14285714]\n",
      " [0.14285714 0.21555515 0.14285714 0.14285714]]\n",
      "loss :  1.3236746715423062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#สร้าง array ใหม่โดยใช้วิธีการหยิบค่าจาก p_correct โดยที่\n",
    "\n",
    "#p_correct[0, 0] หมายถึงเข้าไปหยิบค่ามาจาก P[0, 0, Y[0, 0]]\n",
    "#p_correct[1, 2] หมายถึงเข้าไปหยิบค่ามาจาก P[1, 2, Y[1, 2]]\n",
    "p_correct = P[np.arange(B)[:, None], np.arange(T)[None, :], Y] # (B, T)\n",
    "print(\"p_correct ->\\n\", p_correct);\n",
    "\n",
    "nll = -np.log(p_correct + 1e-12) # เลข 1e-12 ใส่มาเพื่อกัน log (0)\n",
    "loss = (nll * m).sum() / max(M, 1)\n",
    "print(\"loss : \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d4373",
   "metadata": {},
   "source": [
    "#### วิธีการคำนวน A : $\\frac{\\partial \\mathcal {L} } {\\partial Z} = \\frac {\\left(P - Y_{one-hot}\\right)\\ *\\ m[:, :, None] } {M}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4553a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00250992  0.00250992  0.00250992 -0.14958683  0.13703721\n",
      "    0.00250992  0.00250992]\n",
      "  [ 0.0229403   0.0229403   0.0229403   0.0229403  -0.1376418\n",
      "    0.0229403   0.0229403 ]\n",
      "  [ 0.01493757  0.01493757  0.01493757  0.01493757  0.01493757\n",
      "   -0.08962544  0.01493757]\n",
      "  [-0.          0.          0.          0.          0.\n",
      "    0.          0.        ]]\n",
      "\n",
      " [[ 0.02857143  0.02857143  0.02857143  0.02857143 -0.17142857\n",
      "    0.02857143  0.02857143]\n",
      "  [ 0.02614816  0.02614816  0.02614816  0.02614816  0.02614816\n",
      "   -0.15688897  0.02614816]\n",
      "  [-0.          0.          0.          0.          0.\n",
      "    0.          0.        ]\n",
      "  [-0.          0.          0.          0.          0.\n",
      "    0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "dZ = (P - Y_onehot) * m[:, :, None]\n",
    "dZ = dZ / max(M, 1)\n",
    "print (dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81c7827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_mark m = \n",
      " [[1. 1. 1. 0.]\n",
      " [1. 1. 0. 0.]]\n",
      "M =  5.0\n",
      "masked_loss =  1.3236746715423062\n",
      "\n",
      "dZ[0, 0, :] = [ 0.00250992  0.00250992  0.00250992 -0.14958683  0.13703721  0.00250992\n",
      "  0.00250992]\n",
      "dZ[0, 1, :] = [ 0.0229403  0.0229403  0.0229403  0.0229403 -0.1376418  0.0229403\n",
      "  0.0229403]\n",
      "dZ[0, 2, :] = [ 0.01493757  0.01493757  0.01493757  0.01493757  0.01493757 -0.08962544\n",
      "  0.01493757]\n"
     ]
    }
   ],
   "source": [
    "print(\"loss_mark m = \\n\", m) \n",
    "print(\"M = \", M)\n",
    "print(\"masked_loss = \", loss)\n",
    "\n",
    "print(\"\\ndZ[0, 0, :] =\", dZ[0, 0, :])\n",
    "print(\"dZ[0, 1, :] =\", dZ[0,1,:])\n",
    "print(\"dZ[0, 2, :] =\", dZ[0,2,:])  # PAD target -> mask=0 -> should be all zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee57dc1a",
   "metadata": {},
   "source": [
    "จะเห็นได้ว่าตำแหน่งที่ถูก mask ไว้ (Y = PAD) &rarr; $m = 0$  &rarr; $dZ = 0$  \n",
    "\n",
    "ทำให้ทั้ง Vocab &rarr; weight upstream ไม่โดน Update จากตำแหน่งนั้นเลย"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d6d8ed",
   "metadata": {},
   "source": [
    "#### วิธีการคำนวณ B : index trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa195016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 5, 0],\n",
       "       [4, 5, 0, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae55caf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = \n",
      " [[[0.01254962 0.01254962 0.01254962 0.25206586 0.68518604 0.01254962\n",
      "   0.01254962]\n",
      "  [0.1147015  0.1147015  0.1147015  0.1147015  0.311791   0.1147015\n",
      "   0.1147015 ]\n",
      "  [0.07468786 0.07468786 0.07468786 0.07468786 0.07468786 0.55187282\n",
      "   0.07468786]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]\n",
      "\n",
      " [[0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.13074081 0.13074081 0.13074081 0.13074081 0.13074081 0.21555515\n",
      "   0.13074081]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]]\n",
      "\n",
      "\n",
      "dZ => \n",
      " [[[ 0.00250992  0.00250992  0.00250992 -0.14958683  0.13703721\n",
      "    0.00250992  0.00250992]\n",
      "  [ 0.0229403   0.0229403   0.0229403   0.0229403  -0.1376418\n",
      "    0.0229403   0.0229403 ]\n",
      "  [ 0.01493757  0.01493757  0.01493757  0.01493757  0.01493757\n",
      "   -0.08962544  0.01493757]\n",
      "  [-0.          0.          0.          0.          0.\n",
      "    0.          0.        ]]\n",
      "\n",
      " [[ 0.02857143  0.02857143  0.02857143  0.02857143 -0.17142857\n",
      "    0.02857143  0.02857143]\n",
      "  [ 0.02614816  0.02614816  0.02614816  0.02614816  0.02614816\n",
      "   -0.15688897  0.02614816]\n",
      "  [-0.          0.          0.          0.          0.\n",
      "    0.          0.        ]\n",
      "  [-0.          0.          0.          0.          0.\n",
      "    0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "dZ = P.copy()\n",
    "B, T, V = P.shape\n",
    "M = m.sum()\n",
    "\n",
    "\n",
    "print(\"P = \\n\", P)\n",
    "\n",
    "\n",
    "# subtract 1 at the true class indices\n",
    "dZ[np.arange(B)[:, None], np.arange(T)[None, :], Y ] -= 1.0\n",
    "\n",
    "# apply mask and normalize\n",
    "dZ *= m[:, :, None]\n",
    "dZ /= max(M, 1) # avoid division by zero\n",
    "print(\"\\n\\ndZ => \\n\", dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f6b28",
   "metadata": {},
   "source": [
    "### ทำไมตอนใส่ loss mask เราใช้แค่ M ในการ normalize ไม่ใช่ B * T หรือ M * B * T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee746e4b",
   "metadata": {},
   "source": [
    "> เป้าหมายในการ normalize คือให้ loss / gradient เป็น average ต่อ 1 token ที่ถูก train ซึ่งจำนวน token ที่ถูก train คือ $M = \\sum m$\n",
    "\n",
    "**กันสับสน ตำแหน่งที่ถูก train จะมีค่าเท่ากับ 1 ใน loss mask matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03708ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7902c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72edde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99d7c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "loss_mask = np.array([[1, 1, 0], [1, 0, 0]])\n",
    "print(loss_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794e733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3efe72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_head_forward(H, W, b):\n",
    "    #H: (T, d), W: (d, V), b: (V,)\n",
    "    Z = H @ W + b # (T, V) probs\n",
    "    P = softmax(Z, axis = 1) # (T, V) probs\n",
    "\n",
    "    return Z, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "295c097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossAndSoftmaxBackward(y, p):\n",
    "    return p - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83f63a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  0.2, -0.6,  0.1,  0. ,  0.1],\n",
       "       [ 0.1,  0.1,  0.2,  0. ,  4. ,  0.1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlossBydz = lossAndSoftmaxBackward(y, p)\n",
    "dlossBydz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7875f67",
   "metadata": {},
   "source": [
    "## Use in another sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd2972c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00250992,  0.00250992,  0.00250992, -0.14958683,\n",
       "          0.13703721,  0.00250992,  0.00250992],\n",
       "        [ 0.0229403 ,  0.0229403 ,  0.0229403 ,  0.0229403 ,\n",
       "         -0.1376418 ,  0.0229403 ,  0.0229403 ],\n",
       "        [ 0.01493757,  0.01493757,  0.01493757,  0.01493757,\n",
       "          0.01493757, -0.08962544,  0.01493757],\n",
       "        [-0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.02857143,  0.02857143,  0.02857143,  0.02857143,\n",
       "         -0.17142857,  0.02857143,  0.02857143],\n",
       "        [ 0.02614816,  0.02614816,  0.02614816,  0.02614816,\n",
       "          0.02614816, -0.15688897,  0.02614816],\n",
       "        [-0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [-0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5c836",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
